{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RmEkiQUcXMWG"
      },
      "outputs": [],
      "source": [
        "!pip install langchain -q\n",
        "!pip install openai -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit -q\n",
        "!pip install pyngrok -q"
      ],
      "metadata": {
        "id": "9DS3vcolXd3z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-gpu"
      ],
      "metadata": {
        "id": "gfdC-m11gqEv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install --upgrade --quiet  langchain langchain-community langchain-openai"
      ],
      "metadata": {
        "id": "d1vHi09ibEV0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = 'OPENAI_API_KEY'\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = 'LANGCHAIN_KEY'"
      ],
      "metadata": {
        "id": "6YRpNEM8XO2d"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "from langchain_core.messages import HumanMessage,AIMessage\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate,MessagesPlaceholder\n",
        "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "import streamlit as st\n",
        "\n",
        "\n",
        "#Return vectorstore for the URL\n",
        "def get_vector_store(url):\n",
        "  loader = WebBaseLoader(url)\n",
        "  data = loader.load()\n",
        "\n",
        "  text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
        "  chunks = text_splitter.split_documents(data)\n",
        "  vector_store = FAISS.from_documents(chunks, OpenAIEmbeddings())\n",
        "  return vector_store\n",
        "\n",
        "#Returns history_retriever_chain\n",
        "def get_retreiver_chain(vector_store):\n",
        "  llm=ChatOpenAI()\n",
        "  retriever = vector_store.as_retriever()\n",
        "  prompt = ChatPromptTemplate.from_messages([\n",
        "      MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "      (\"user\",\"{input}\"),\n",
        "      (\"user\",\"Given the above conversation, generate a search query to look up in order to get information relevant to the conversation\")\n",
        "  ])\n",
        "  history_retriver_chain = create_history_aware_retriever(llm,retriever,prompt)\n",
        "\n",
        "  return history_retriver_chain\n",
        "\n",
        "#Returns conversational rag\n",
        "def get_conversational_rag(history_retriever_chain):\n",
        "  llm=ChatOpenAI()\n",
        "  answer_prompt=ChatPromptTemplate.from_messages([\n",
        "      (\"system\",\"Answer the user's questions based on the below context:\\n\\n{context}\"),\n",
        "      MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "      (\"user\",\"{input}\")\n",
        "  ])\n",
        "\n",
        "  document_chain = create_stuff_documents_chain(llm,answer_prompt)\n",
        "\n",
        "  #create final retrieval chain\n",
        "  conversational_retrieval_chain = create_retrieval_chain(history_retriever_chain,document_chain)\n",
        "\n",
        "  return conversational_retrieval_chain\n",
        "\n",
        "#Returns th final response\n",
        "def get_response(user_input):\n",
        "  history_retriever_chain = get_retreiver_chain(st.session_state.vector_store)\n",
        "  conversation_rag_chain = get_conversational_rag(history_retriever_chain)\n",
        "  response = conversation_rag_chain.invoke({\n",
        "        \"chat_history\":st.session_state.chat_history,\n",
        "        \"input\":user_input\n",
        "    })\n",
        "  return response[\"answer\"]\n",
        "\n",
        "\n",
        "\n",
        "#Streamlit app\n",
        "\n",
        "st.header(\"Chat with websites\")\n",
        "\n",
        "chat_history=[]\n",
        "vector_store=[]\n",
        "\n",
        "\n",
        "# Sidebar\n",
        "# URL pasting in sidebar on the left\n",
        "with st.sidebar:\n",
        "  st.header(\"Paste your URL\")\n",
        "  website_url = st.text_input(\"Enter URL\")\n",
        "\n",
        "if website_url is None or website_url.strip()==\"\":\n",
        "  st.info(\"Please enter a website URL\")\n",
        "else:\n",
        "  #session state\n",
        "  if \"chat_history\" not in st.session_state:\n",
        "    st.session_state.chat_history=[\n",
        "        AIMessage(content=\"I am a bot, how can I help you?\")\n",
        "    ]\n",
        "   #create conversation chain\n",
        "  if vector_store not in st.session_state:\n",
        "      st.session_state.vector_store = get_vector_store(website_url)\n",
        "\n",
        "  user_input=st.chat_input(\"Type your message here...\")\n",
        "  if user_input is not None and user_input.strip()!=\"\":\n",
        "    response = get_response(user_input)\n",
        "\n",
        "    st.session_state.chat_history.append(HumanMessage(content=user_input))\n",
        "    st.session_state.chat_history.append(AIMessage(content=response))\n",
        "\n",
        "  for message in st.session_state.chat_history:\n",
        "      if isinstance(message,AIMessage):\n",
        "        with st.chat_message(\"AI\"):\n",
        "          st.write(message.content)\n",
        "      else:\n",
        "        with st.chat_message(\"Human\"):\n",
        "          st.write(message.content)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kr0iND1AYK9T",
        "outputId": "957ea8e1-26c0-477a-aafe-82fe1781c8ad"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "import time\n",
        "\n",
        "#Set auth token\n",
        "ngrok.set_auth_token(\"2c54S0BCzBB0ZiGw5FSS82ulUNb_7wsNuyyPb22woziTCnnXn\")\n",
        "\n",
        "\n",
        "#time.sleep(2)\n",
        "\n",
        "!nohup streamlit run app.py --server.port 5011 &\n",
        "# Start ngrok tunnel\n",
        "ngrok_tunnel = ngrok.connect(addr='5011', proto='http', bind_tls=True)\n",
        "\n",
        "# Print the URL\n",
        "print(' * Tunnel URL:', ngrok_tunnel.public_url)\n",
        "\n",
        "# Keep the Colab session alive\n",
        "#while True:\n",
        "#    time.sleep(120)  # Keep the session alive for 120sec"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5rQF8OxXRaJ",
        "outputId": "b2bf49a9-81bd-4d95-ab15-4d27958729b6"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nohup: appending output to 'nohup.out'\n",
            " * Tunnel URL: https://58ed-35-243-131-19.ngrok-free.app\n"
          ]
        }
      ]
    }
  ]
}