{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bOZPhXanAl5P"
      },
      "outputs": [],
      "source": [
        "!pip install langchain-q\n",
        "!pip install openai\n",
        "%pip install --upgrade --quiet  langchain langchain-community langchain-openai\n",
        "!pip install langchain-community --upgrade\n",
        "!pip install langchain psycopg2-binary sqlalchemy\n",
        "!pip install langchain-community langchain\n",
        "!pip install langchain-experimental\n",
        "!pip install faiss-cpu\n",
        "!pip install streamlit -q\n",
        "!pip install pyngrok -q\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = 'OPENAI_API_KEY'"
      ],
      "metadata": {
        "id": "cVbH36p7A5Rq"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "from langchain_core.messages import HumanMessage,AIMessage\n",
        "from langchain_community.utilities import SQLDatabase\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate,MessagesPlaceholder\n",
        "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain.agents.agent_toolkits import create_sql_agent\n",
        "from langchain.sql_database import SQLDatabase\n",
        "from langchain.agents import AgentExecutor\n",
        "from langchain.utilities.sql_database import SQLDatabase\n",
        "from langchain_core.documents import Document\n",
        "import pandas as pd\n",
        "import datetime\n",
        "import streamlit as st\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def get_data():\n",
        "    # Connect to Neon PostgreSQL\n",
        "    neon_connection_uri = \"postgresql://neondb_owner:npg_TLwCWXpuP61F@ep-mute-dew-a11ioqrm-pooler.ap-southeast-1.aws.neon.tech/neondb?sslmode=require\"\n",
        "    db = SQLDatabase.from_uri(neon_connection_uri)\n",
        "\n",
        "    query = \"SELECT id, first_name, last_name, email,hire_date FROM employees\"\n",
        "    rows = db._execute(query)\n",
        "\n",
        "    return rows\n",
        "\n",
        "def get_documents(rows):\n",
        "    documents = []\n",
        "    for row in rows:\n",
        "      content = f\"id: {row['id']}, name: {row['first_name']} {row['last_name']}, email: {row['email']}, hire_date:{row['hire_date']}\"\n",
        "      documents.append(Document(page_content=content, metadata={\"source\": \"employees\"}))\n",
        "\n",
        "    return documents\n",
        "\n",
        "#Set up Vectorstore\n",
        "def get_vector_store(documents):\n",
        "  embedding_model = OpenAIEmbeddings()\n",
        "  vectorstore = FAISS.from_documents(documents, embedding_model)\n",
        "  return vectorstore\n",
        "\n",
        "#Returns history_retriever_chain\n",
        "def get_retreiver_chain(vector_store):\n",
        "  llm=ChatOpenAI()\n",
        "  retriever = vector_store.as_retriever()\n",
        "  prompt = ChatPromptTemplate.from_messages([\n",
        "      MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "      (\"user\",\"{input}\"),\n",
        "      (\"user\",\"Given the above conversation, generate a search query to look up in order to get information relevant to the conversation\")\n",
        "  ])\n",
        "  history_retriver_chain = create_history_aware_retriever(llm,retriever,prompt)\n",
        "\n",
        "  return history_retriver_chain\n",
        "\n",
        "#Returns conversational rag\n",
        "def get_conversational_rag(history_retriever_chain):\n",
        "  llm=ChatOpenAI()\n",
        "  answer_prompt=ChatPromptTemplate.from_messages([\n",
        "      (\"system\",\"Answer the user's questions based on the below context:\\n\\n{context}\"),\n",
        "      MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "      (\"user\",\"{input}\")\n",
        "  ])\n",
        "\n",
        "  document_chain = create_stuff_documents_chain(llm,answer_prompt)\n",
        "\n",
        "  #create final retrieval chain\n",
        "  conversational_retrieval_chain = create_retrieval_chain(history_retriever_chain,document_chain)\n",
        "\n",
        "  return conversational_retrieval_chain\n",
        "\n",
        "\n",
        "#Returns th final response\n",
        "def get_response(user_input):\n",
        "  history_retriever_chain = get_retreiver_chain(st.session_state.vector_store)\n",
        "  conversation_rag_chain = get_conversational_rag(history_retriever_chain)\n",
        "  response = conversation_rag_chain.invoke({\n",
        "        \"chat_history\":st.session_state.chat_history,\n",
        "        \"input\":user_input\n",
        "    })\n",
        "  return response[\"answer\"]\n",
        "\n",
        "#Streamlit app\n",
        "\n",
        "st.header(\"Chat with Database\")\n",
        "\n",
        "chat_history=[]\n",
        "vector_store=[]\n",
        "\n",
        "\n",
        "# Sidebar\n",
        "# URL pasting in sidebar on the left\n",
        "with st.sidebar:\n",
        "  st.header(\"Are you a valid user\")\n",
        "  password = st.text_input(\"Enter Password\")\n",
        "\n",
        "if password != \"SQL\":\n",
        "  st.write(\"Please enter a valid password\")\n",
        "else:\n",
        "  if \"chat_history\" not in st.session_state:\n",
        "    st.session_state.chat_history=[\n",
        "        AIMessage(content=\"I am a bot, how can I help you?\")\n",
        "    ]\n",
        "\n",
        "  #create conversation chain\n",
        "  if vector_store not in st.session_state:\n",
        "    rows = get_data()\n",
        "\n",
        "    #***********\n",
        "    # Convert to DataFrame\n",
        "    df = pd.DataFrame(rows)\n",
        "\n",
        "    # Optional: format the full name\n",
        "    df['Name'] = df['first_name'] + \" \" + df['last_name']\n",
        "\n",
        "    # Reorder columns\n",
        "    df = df[['id', 'Name', 'email', 'hire_date']]\n",
        "\n",
        "    # Display in sidebar as a table\n",
        "    st.sidebar.title(\"Employee Directory\")\n",
        "    st.sidebar.dataframe(df)\n",
        "    #***********\n",
        "\n",
        "\n",
        "    documents = get_documents(rows)\n",
        "    st.session_state.vector_store = get_vector_store(documents)\n",
        "\n",
        "  user_input=st.chat_input(\"Type your message here...\")\n",
        "  if user_input is not None and user_input.strip()!=\"\":\n",
        "    response = get_response(user_input)\n",
        "\n",
        "    st.session_state.chat_history.append(HumanMessage(content=user_input))\n",
        "    st.session_state.chat_history.append(AIMessage(content=response))\n",
        "\n",
        "  for message in st.session_state.chat_history:\n",
        "    if isinstance(message,AIMessage):\n",
        "      with st.chat_message(\"AI\"):\n",
        "        st.write(message.content)\n",
        "    else:\n",
        "      with st.chat_message(\"Human\"):\n",
        "        st.write(message.content)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hc53UL4vBDYU",
        "outputId": "9ce7f054-bbca-4395-aba0-d1637fa2862f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "import time\n",
        "\n",
        "#Set auth token\n",
        "ngrok.set_auth_token(\"ngrok_KEY\")\n",
        "\n",
        "\n",
        "#time.sleep(2)\n",
        "\n",
        "!nohup streamlit run app.py --server.port 5011 &\n",
        "# Start ngrok tunnel\n",
        "ngrok_tunnel = ngrok.connect(addr='5011', proto='http', bind_tls=True)\n",
        "\n",
        "# Print the URL\n",
        "print(' * Tunnel URL:', ngrok_tunnel.public_url)\n",
        "\n",
        "# Keep the Colab session alive\n",
        "#while True:\n",
        "#    time.sleep(120)  # Keep the session alive for 120sec"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O0g260m8BY0Z",
        "outputId": "7aa29b74-9118-4c9f-b694-3e25a9a9f926"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nohup: appending output to 'nohup.out'\n",
            " * Tunnel URL: https://847e-34-16-250-242.ngrok-free.app\n"
          ]
        }
      ]
    }
  ]
}