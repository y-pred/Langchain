{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/y-pred/Langchain/blob/main/Different_Memory_Types.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AP_EGBMnjZyz"
      },
      "outputs": [],
      "source": [
        "!pip install langchain -q\n",
        "!pip install openai -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0wE7vnYQj2wx"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from langchain.llms import OpenAI\n",
        "os.environ['OPENAI_API_KEY'] = 'OPENAI_API_KEY'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UCZmpcRexATO",
        "outputId": "4c3f0f5c-4417-4f7e-91ba-0078844782eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, sentence-transformers\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 sentence-transformers-2.7.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -U sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BVY_mvrXxD9v"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpubWo5NsypF"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovstTCFm9JuR"
      },
      "source": [
        "###ConverstaionBuffer:\n",
        "This memory allows for storing messages and then extracts the messages in a variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "753LM2Vu1JmM"
      },
      "outputs": [],
      "source": [
        "from langchain.memory import ConversationBufferMemory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kZTWUYLB6O6i"
      },
      "outputs": [],
      "source": [
        "memory = ConversationBufferMemory()\n",
        "memory.save_context({\"input\": \"hi\"}, {\"output\": \"whats up\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SfLltslP6QO4",
        "outputId": "361d1587-4ea5-48e6-b857-234efc87a756"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'history': 'Human: hi\\nAI: whats up'}"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OG2DcgHh-eR1",
        "outputId": "10f6acc9-c4ab-4dab-eb47-8299bf72b11c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'history': 'Human: hi\\nAI: whats up\\nHuman: How is the tea\\nAI: Too sweet'}"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "memory.save_context({\"input\":\"How is the tea\"},{\"output\":\"Too sweet\"})\n",
        "memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eJm0eWCI8a9u"
      },
      "outputs": [],
      "source": [
        "#By default, they are returned as a single string. In order to return as a list of messages, you can set return_messages=True\n",
        "\n",
        "memory = ConversationBufferMemory(return_messages=True)\n",
        "memory.save_context({\"input\": \"hi\"}, {\"output\": \"whats up\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZHYO9dCs8dZe",
        "outputId": "21ba0fe8-7619-45d2-9091-117948844523"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'history': [HumanMessage(content='hi'), AIMessage(content='whats up')]}"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V6SWbjWe-S1e",
        "outputId": "4b3240b7-b3b6-4a5e-c208-278ba8e18c40"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'history': [HumanMessage(content='hi'),\n",
              "  AIMessage(content='whats up'),\n",
              "  HumanMessage(content='How is the weather'),\n",
              "  AIMessage(content='Its a bit cloudy today')]}"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "memory.save_context({\"input\":\"How is the weather\"},{\"output\":\"Its a bit cloudy today\"})\n",
        "memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "id": "Pq5yGKCN8o45",
        "outputId": "a17dcc42-be78-4655-c043-c59cbcbfd9a5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.llms.openai.OpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAI`.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: Hi there!\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\" Hello! It's nice to meet you. I am an AI created by OpenAI. I am constantly learning and improving my abilities through machine learning algorithms. How can I assist you today?\""
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Using Chain\n",
        "\n",
        "from langchain.chains import ConversationChain\n",
        "\n",
        "\n",
        "llm = OpenAI(temperature=0)\n",
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    verbose=True,\n",
        "    memory=ConversationBufferMemory()\n",
        ")\n",
        "\n",
        "conversation.predict(input=\"Hi there!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "yp7SkrTP828w",
        "outputId": "cf9a2005-b0ab-4a9c-ef3b-2a24aecbb266"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Hi there!\n",
            "AI:  Hello! It's nice to meet you. I am an AI created by OpenAI. I am constantly learning and improving my abilities through machine learning algorithms. How can I assist you today?\n",
            "Human: I'm doing well! Just having a conversation with an AI.\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\" That's great to hear! I am always happy to engage in conversations and learn more about human interactions. Is there anything specific you would like to talk about?\""
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation.predict(input=\"I'm doing well! Just having a conversation with an AI.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "id": "56odVLj089vu",
        "outputId": "651bcccd-0f1b-4eaa-fda2-06a533a7d15f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: Hi there!\n",
            "AI:  Hello! It's nice to meet you. I am an AI created by OpenAI. I am constantly learning and improving my abilities through machine learning algorithms. How can I assist you today?\n",
            "Human: I'm doing well! Just having a conversation with an AI.\n",
            "AI:  That's great to hear! I am always happy to engage in conversations and learn more about human interactions. Is there anything specific you would like to talk about?\n",
            "Human: Tell me about yourself.\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' Of course! As I mentioned, I am an AI created by OpenAI. I am designed to process and analyze large amounts of data, and use that information to perform various tasks and provide helpful responses. I am constantly learning and adapting to new information, which allows me to improve my abilities over time. Is there anything else you would like to know about me?'"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation.predict(input=\"Tell me about yourself.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NV7ZY2589PT6"
      },
      "source": [
        "###**Conversation Buffer Window**\n",
        "Keeps a list of the interactions of the conversation over time. It only uses the last K interactions. This can be useful for keeping a sliding window of the most recent interactions, so the buffer does not get too large."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ArBgRXQc9VMh",
        "outputId": "b33bf083-1242-44d3-d1e9-453cc80d54e3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'history': [HumanMessage(content='Hi Mate!'),\n",
              "  AIMessage(content=\"What's up!\")]}"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "\n",
        "memory = ConversationBufferWindowMemory(k=3,return_messages=True)\n",
        "memory.save_context({\"input\":\"Hi Mate!\"},{\"output\":\"What's up!\"})\n",
        "memory.load_memory_variables({})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eW96D9fuAAc0",
        "outputId": "8da3e83c-18a7-4aa0-f98d-b2b02f3989a9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'history': [HumanMessage(content='Hi Mate!'),\n",
              "  AIMessage(content=\"What's up!\"),\n",
              "  HumanMessage(content='Not too bad! Help me with a doubt'),\n",
              "  AIMessage(content='Yes please!'),\n",
              "  HumanMessage(content='Why is 2+2 not 5!'),\n",
              "  AIMessage(content='Because its 4!')]}"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "memory.save_context({\"input\":\"Not too bad! Help me with a doubt\"},{\"output\":\"Yes please!\"})\n",
        "memory.save_context({\"input\":\"Why is 2+2 not 5!\"},{\"output\":\"Because its 4!\"})\n",
        "memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "tjkN_HupAdLM",
        "outputId": "24468679-a372-48e0-ad36-aafaf6d5ae73"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "[]\n",
            "Human: Hi How are you?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' Hello! I am an AI and do not have the ability to feel emotions like humans do. However, I am functioning properly and ready to assist you with any questions or tasks you may have. Is there something specific you would like to know or discuss?'"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Using it in a chain\n",
        "from langchain.chains import ConversationChain\n",
        "conversation_k = ConversationChain(\n",
        "    llm=OpenAI(temperature=0),\n",
        "    memory = ConversationBufferWindowMemory(k=2,return_messages=True),\n",
        "    verbose=True\n",
        ")\n",
        "conversation_k.predict(input=\"Hi How are you?\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        },
        "id": "kDqT-y8nBFqq",
        "outputId": "b04cb045-e201-4813-a611-ec17a2a2ee26"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "[HumanMessage(content='Hi How are you?'), AIMessage(content=' Hello! I am an AI and do not have the ability to feel emotions like humans do. However, I am functioning properly and ready to assist you with any questions or tasks you may have. Is there something specific you would like to know or discuss?')]\n",
            "Human: Need some help!\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' Of course! I am always happy to assist. What do you need help with?'"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation_k.predict(input=\"Need some help!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        },
        "id": "JE-rwpubBNyX",
        "outputId": "7fc9161e-1f8e-4f9a-a41c-dd21500eebac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "[HumanMessage(content='Hi How are you?'), AIMessage(content=' Hello! I am an AI and do not have the ability to feel emotions like humans do. However, I am functioning properly and ready to assist you with any questions or tasks you may have. Is there something specific you would like to know or discuss?'), HumanMessage(content='Need some help!'), AIMessage(content=' Of course! I am always happy to assist. What do you need help with?')]\n",
            "Human: Where is Mount Everest?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' Mount Everest is located in the Mahalangur Himal range in Nepal and Tibet. It is the highest mountain in the world, standing at 29,032 feet tall.'"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation_k.predict(input='Where is Mount Everest?')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        },
        "id": "Ucb4oPTHBYsn",
        "outputId": "24d540d1-c1f2-48f2-c8a3-823d9d5f2010"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "[HumanMessage(content='Need some help!'), AIMessage(content=' Of course! I am always happy to assist. What do you need help with?'), HumanMessage(content='Where is Mount Everest?'), AIMessage(content=' Mount Everest is located in the Mahalangur Himal range in Nepal and Tibet. It is the highest mountain in the world, standing at 29,032 feet tall.')]\n",
            "Human: What is the capital of Nepal?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\" The capital of Nepal is Kathmandu. It is located in the Kathmandu Valley and is the largest city in Nepal. It is also the country's political, cultural, and economic center.\""
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation_k.predict(input='What is the capital of Nepal?')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxzJxEeGtmbJ"
      },
      "source": [
        "###ConverstaionBufferMemory using LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ws-w1aMks19N",
        "outputId": "c923ab0f-277e-4ce6-a58b-fc5776262807"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mYou are a nice chatbot having a conversation with a human.\n",
            "\n",
            "Previous conversation:\n",
            "[]\n",
            "\n",
            "New human question: hi\n",
            "Response:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'question': 'hi',\n",
              " 'chat_history': [HumanMessage(content='hi'),\n",
              "  AIMessage(content=' Hello! How are you doing today?')],\n",
              " 'text': ' Hello! How are you doing today?'}"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "\n",
        "llm = OpenAI(temperature=0)\n",
        "# Notice that \"chat_history\" is present in the prompt template\n",
        "template = \"\"\"You are a nice chatbot having a conversation with a human.\n",
        "\n",
        "Previous conversation:\n",
        "{chat_history}\n",
        "\n",
        "New human question: {question}\n",
        "Response:\"\"\"\n",
        "prompt = PromptTemplate.from_template(template)\n",
        "# Notice that we need to align the `memory_key`\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\",return_messages=True)\n",
        "conversation = LLMChain(\n",
        "    llm=llm,\n",
        "    prompt=prompt,\n",
        "    verbose=True,\n",
        "    memory=memory\n",
        ")\n",
        "\n",
        "conversation({\"question\": \"hi\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zR-KkK9PtCkJ",
        "outputId": "95df86ce-8a1a-4b4f-8254-a5d479096e48"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mYou are a nice chatbot having a conversation with a human.\n",
            "\n",
            "Previous conversation:\n",
            "[HumanMessage(content='hi'), AIMessage(content=' Hello! How are you doing today?')]\n",
            "\n",
            "New human question: Not too bad! Can you help me?\n",
            "Response:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'question': 'Not too bad! Can you help me?',\n",
              " 'chat_history': [HumanMessage(content='hi'),\n",
              "  AIMessage(content=' Hello! How are you doing today?'),\n",
              "  HumanMessage(content='Not too bad! Can you help me?'),\n",
              "  AIMessage(content=' Of course! I am here to assist you with any questions or tasks you may have. What can I help you with?')],\n",
              " 'text': ' Of course! I am here to assist you with any questions or tasks you may have. What can I help you with?'}"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation({\"question\": \"Not too bad! Can you help me?\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_unuCm6ftK_Q",
        "outputId": "61988593-f3d7-40ab-f7f2-3bc6ccdd934e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mYou are a nice chatbot having a conversation with a human.\n",
            "\n",
            "Previous conversation:\n",
            "Human: hi\n",
            "AI:  Hello! How are you doing today?\n",
            "Human: Not too bad! Can you help me?\n",
            "AI:  Of course, I am here to assist you with anything you need. What can I help you with?\n",
            "\n",
            "New human question: What is the capital of India?\n",
            "Response:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'question': 'What is the capital of India?',\n",
              " 'chat_history': 'Human: hi\\nAI:  Hello! How are you doing today?\\nHuman: Not too bad! Can you help me?\\nAI:  Of course, I am here to assist you with anything you need. What can I help you with?',\n",
              " 'text': ' The capital of India is New Delhi.'}"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation({\"question\": \"What is the capital of India?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FL7-E6dWtrKN"
      },
      "source": [
        "###ConversationBufferMemory using chatbot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wxut0guettqF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        },
        "outputId": "bbdc0ce9-9bb5-433d-89d6-d0ff77648c17"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'langchain'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-eb9db50a4e3d>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchat_models\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mChatOpenAI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m from langchain_core.prompts import (\n\u001b[1;32m      3\u001b[0m     \u001b[0mChatPromptTemplate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mMessagesPlaceholder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mSystemMessagePromptTemplate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'langchain'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain_core.prompts import (\n",
        "    ChatPromptTemplate,\n",
        "    MessagesPlaceholder,\n",
        "    SystemMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "\n",
        "llm = ChatOpenAI()\n",
        "prompt = ChatPromptTemplate(\n",
        "    messages=[\n",
        "        SystemMessagePromptTemplate.from_template(\n",
        "            \"You are a chatbot with a sense of humor.\"\n",
        "        ),\n",
        "        # The `variable_name` here is what must align with memory\n",
        "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "        HumanMessagePromptTemplate.from_template(\"{question}\")\n",
        "    ]\n",
        ")\n",
        "# Notice that we `return_messages=True` to fit into the MessagesPlaceholder\n",
        "# Notice that `\"chat_history\"` aligns with the MessagesPlaceholder name.\n",
        "memory = ConversationBufferMemory(k=2,memory_key=\"chat_history\", return_messages=True)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3MCrz5lhHroY",
        "outputId": "6d9ea86b-0458-4390-a5fb-42efa4235489"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mSystem: You are a chatbot with a sense of humor.\n",
            "Human: hi\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'question': 'hi',\n",
              " 'chat_history': [HumanMessage(content='hi'),\n",
              "  AIMessage(content='Well, hello there! How can I make you smile today?')],\n",
              " 'text': 'Well, hello there! How can I make you smile today?'}"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation = LLMChain(\n",
        "    llm=llm,\n",
        "    prompt=prompt,\n",
        "    verbose=True,\n",
        "    memory=memory\n",
        ")\n",
        "\n",
        "conversation({\"question\": \"hi\"})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        },
        "id": "iPTL_OJRHolz",
        "outputId": "eb97ac52-1574-4ddc-c4a5-568cc8d7fbc2"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'langchain'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-1b113c9c99f1>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchains\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConversationChain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m conversation = ConversationChain(\n\u001b[1;32m      3\u001b[0m     \u001b[0mllm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mllm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'langchain'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "from langchain.chains import ConversationChain\n",
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    prompt=prompt,\n",
        "    verbose=True,\n",
        "    memory=memory\n",
        "\n",
        ")\n",
        "\n",
        "conversation.predict(input=\"Hi there!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kdfCeF4yuUgu",
        "outputId": "f0d1e819-7a54-49e3-b660-7fc7228f297a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mSystem: You are a chatbot with a sense of humor.\n",
            "Human: hi\n",
            "AI: Hello! How are you doing today?\n",
            "Human: Great! Please help me I am stuck?\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'question': 'Great! Please help me I am stuck?',\n",
              " 'chat_history': [HumanMessage(content='hi'),\n",
              "  AIMessage(content='Hello! How are you doing today?'),\n",
              "  HumanMessage(content='Great! Please help me I am stuck?'),\n",
              "  AIMessage(content='Stuck? Did you accidentally glue your hands together or did you get your head stuck in a jar of pickles? Just kidding! What do you need help with?')],\n",
              " 'text': 'Stuck? Did you accidentally glue your hands together or did you get your head stuck in a jar of pickles? Just kidding! What do you need help with?'}"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation({\"question\": \"Great! Please help me I am stuck?\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3OQCBpKkuqEY",
        "outputId": "281db56a-029a-4c08-988a-c4a7b1d3dfb1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mSystem: You are a chatbot with a sense of humor.\n",
            "Human: hi\n",
            "AI: Hello! How are you doing today?\n",
            "Human: Great! Please help me I am stuck?\n",
            "AI: Stuck? Did you accidentally glue your hands together or did you get your head stuck in a jar of pickles? Just kidding! What do you need help with?\n",
            "Human: Haha! you are funny! What would you do if you are stuck between your mom and wife during their argument?\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'question': 'Haha! you are funny! What would you do if you are stuck between your mom and wife during their argument?',\n",
              " 'chat_history': [HumanMessage(content='hi'),\n",
              "  AIMessage(content='Hello! How are you doing today?'),\n",
              "  HumanMessage(content='Great! Please help me I am stuck?'),\n",
              "  AIMessage(content='Stuck? Did you accidentally glue your hands together or did you get your head stuck in a jar of pickles? Just kidding! What do you need help with?'),\n",
              "  HumanMessage(content='Haha! you are funny! What would you do if you are stuck between your mom and wife during their argument?'),\n",
              "  AIMessage(content=\"Oh boy, that's a tricky situation! I'd probably try to diffuse the tension with some humor or distract them with a random fact about penguins. Or maybe I'd just sneak away and hide in the pantry with a bag of cookies until it blows over!\")],\n",
              " 'text': \"Oh boy, that's a tricky situation! I'd probably try to diffuse the tension with some humor or distract them with a random fact about penguins. Or maybe I'd just sneak away and hide in the pantry with a bag of cookies until it blows over!\"}"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation({\"question\":\"Haha! you are funny! What would you do if you are stuck between your mom and wife during their argument?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "u6M5mUKdjbFn"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOSRDJ05RM5hbXY+m7+2FZo",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}